{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Owen Benner Econ 484 Kaggle comp assignment\n",
        "\"\"\"\n",
        "1. Data Processing:\n",
        "- Combined train/test datasets\n",
        "- Handled missing values (median for numeric, mode for categorical)\n",
        "- Converted categorical variables to numeric using LabelEncoder\n",
        "- Created new features (TotalSF, TotalBathrooms, HouseAge, TotalPorchSF)\n",
        "\n",
        "2. Base Models Performance (RMSLE scores - lower is better):\n",
        "- CatBoost:  0.120249 (Best)\n",
        "- XGBoost:   0.126613\n",
        "- LightGBM:  0.132586\n",
        "\n",
        "3. Stacking Implementation:\n",
        "- Used all 3 gradient boosting models as base models\n",
        "- Used Lasso as meta-model\n",
        "- Final stacking score: 0.123707\n",
        " (better than XGBoost and LightGBM but not CatBoost)\n",
        "\"\"\"\n",
        "!pip install catboost\n",
        "#preamble\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import catboost as cb\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import mean_squared_log_error\n",
        "from sklearn.base import BaseEstimator, RegressorMixin, clone\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "NfC90uL2WDxf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "271d839e-9dd9-41f6-c43d-5e789e245ecd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: catboost in /usr/local/lib/python3.10/dist-packages (1.2.7)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.13.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.2.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (9.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "X4xvV3JMs6wP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1461d9bd-6e6e-4105-e59e-555855ffc752"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (1460, 84)\n",
            "Test shape: (1459, 84)\n"
          ]
        }
      ],
      "source": [
        "train_df=pd.read_csv('train.csv')\n",
        "test_df=pd.read_csv('test.csv')\n",
        "\n",
        "#combine train and test\n",
        "def combine_data(train_df, test_df):\n",
        "    train_target = train_df['SalePrice']\n",
        "    train_df = train_df.drop('SalePrice', axis=1)\n",
        "    all_data = pd.concat([train_df, test_df], axis=0)\n",
        "    return all_data, train_target\n",
        "\n",
        "#preprocessing\n",
        "def preprocess_data(df):\n",
        "    #handling missing values\n",
        "    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "    #filling numeric missing values with median\n",
        "    for col in numeric_cols:\n",
        "        df[col] = df[col].fillna(df[col].median())\n",
        "    #filling categorical missing values with mode\n",
        "    for col in categorical_cols:\n",
        "        df[col] = df[col].fillna(df[col].mode()[0])\n",
        "    #converting categorical to numeric using label encoding\n",
        "    from sklearn.preprocessing import LabelEncoder\n",
        "    le = LabelEncoder()\n",
        "    for col in categorical_cols:\n",
        "        df[col] = le.fit_transform(df[col].astype(str))\n",
        "    return df\n",
        "\n",
        "#time for some feature engineering\n",
        "def add_features(df):\n",
        "    # Total SF\n",
        "    df['TotalSF'] = df['TotalBsmtSF'] + df['GrLivArea']\n",
        "    # Total Bathrooms\n",
        "    df['TotalBathrooms'] = (df['FullBath'] + 0.5*df['HalfBath'] + df['BsmtFullBath'] + 0.5*df['BsmtHalfBath'])\n",
        "    # House Age when sold\n",
        "    df['HouseAge'] = df['YrSold'] - df['YearBuilt']\n",
        "    # Total Porch SF\n",
        "    df['TotalPorchSF'] = (df['WoodDeckSF'] + df['OpenPorchSF'] + df['EnclosedPorch'] + df['3SsnPorch'] + df['ScreenPorch'])\n",
        "    return df\n",
        "\n",
        "#applying these functions to the training and test data\n",
        "all_data, target = combine_data(train_df, test_df)\n",
        "all_data = preprocess_data(all_data)\n",
        "all_data = add_features(all_data)\n",
        "\n",
        "#splitting back into train and test\n",
        "train_data = all_data.iloc[:len(train_df)]\n",
        "test_data = all_data.iloc[len(train_df):]\n",
        "\n",
        "print(\"Train shape:\", train_data.shape)\n",
        "print(\"Test shape:\", test_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#XGBoost params\n",
        "xgb_params = {\n",
        "    'learning_rate': 0.01,\n",
        "    'n_estimators': 3000,\n",
        "    'max_depth': 6,\n",
        "    'min_child_weight': 1,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'objective': 'reg:squarederror',\n",
        "    'random_state': 42 }\n",
        "\n",
        "#LightGBM params\n",
        "lgb_params = {\n",
        "    'learning_rate': 0.01,\n",
        "    'n_estimators': 3000,\n",
        "    'num_leaves': 31,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'random_state': 42 }\n",
        "\n",
        "#CatBoost params\n",
        "cat_params = {\n",
        "    'learning_rate': 0.01,\n",
        "    'iterations': 3000,\n",
        "    'depth': 6,\n",
        "    'random_state': 42 }\n",
        "\n",
        "#instantiate models\n",
        "xgb_model = xgb.XGBRegressor(**xgb_params)\n",
        "lgb_model = lgb.LGBMRegressor(**lgb_params)\n",
        "cat_model = CatBoostRegressor(**cat_params, verbose=False)\n",
        "\n",
        "#take a look at preliminary performance\n",
        "def rmsle_cv(model, X, y):\n",
        "    kf = KFold(n_splits=10, shuffle=True, random_state=484)\n",
        "    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_log_error\", cv=kf))\n",
        "    return rmse.mean()\n",
        "print(\"XGBoost score:\", rmsle_cv(xgb_model, train_data, target))\n",
        "print(\"LightGBM score:\", rmsle_cv(lgb_model, train_data, target))\n",
        "print(\"CatBoost score:\", rmsle_cv(cat_model, train_data, target))"
      ],
      "metadata": {
        "id": "ewoP_SxwdjJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lets stack the models!\n",
        "class StackingRegressor(BaseEstimator, RegressorMixin):\n",
        "    def __init__(self, base_models, meta_model, n_folds=5):\n",
        "        self.base_models = base_models\n",
        "        self.meta_model = meta_model\n",
        "        self.n_folds = n_folds\n",
        "        self.base_predictions = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        #generate oof predictions for stacking data\n",
        "        kf = KFold(n_splits=self.n_folds, shuffle=True, random_state=42)\n",
        "        self.base_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
        "\n",
        "        #train each base model using k-fold\n",
        "        for i, model in enumerate(self.base_models):\n",
        "            print(f\"Training base model {i+1}/{len(self.base_models)}\")\n",
        "            for train_idx, val_idx in kf.split(X):\n",
        "                #clone model to ensure fresh instance\n",
        "                model_clone = clone(model)\n",
        "                #train on training fold\n",
        "                model_clone.fit(X.iloc[train_idx], y.iloc[train_idx])\n",
        "                #predict on validation fold\n",
        "                self.base_predictions[val_idx, i] = model_clone.predict(X.iloc[val_idx])\n",
        "\n",
        "            #retrain base model on full dataset\n",
        "            model.fit(X, y)\n",
        "\n",
        "        #train meta model using base model predictions\n",
        "        self.meta_model.fit(self.base_predictions, y)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        #make predictions with base models\n",
        "        meta_features = np.column_stack([\n",
        "            model.predict(X) for model in self.base_models\n",
        "        ])\n",
        "        #use meta model for final predictions\n",
        "        return self.meta_model.predict(meta_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjzHCxYpVsS1",
        "outputId": "a8c14a3d-7a6e-4665-8f8a-4727a440d661"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training base model 1/3\n",
            "Training base model 2/3\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000546 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3777\n",
            "[LightGBM] [Info] Number of data points in the train set: 934, number of used features: 75\n",
            "[LightGBM] [Info] Start training from score 181296.608137\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001068 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3757\n",
            "[LightGBM] [Info] Number of data points in the train set: 934, number of used features: 74\n",
            "[LightGBM] [Info] Start training from score 181116.488223\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000826 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3756\n",
            "[LightGBM] [Info] Number of data points in the train set: 934, number of used features: 75\n",
            "[LightGBM] [Info] Start training from score 182048.067452\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000546 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3753\n",
            "[LightGBM] [Info] Number of data points in the train set: 935, number of used features: 75\n",
            "[LightGBM] [Info] Start training from score 181600.441711\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000468 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3758\n",
            "[LightGBM] [Info] Number of data points in the train set: 935, number of used features: 75\n",
            "[LightGBM] [Info] Start training from score 181146.250267\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000603 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3986\n",
            "[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 77\n",
            "[LightGBM] [Info] Start training from score 181441.541952\n",
            "Training base model 3/3\n",
            "Training base model 1/3\n",
            "Training base model 2/3\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000516 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3755\n",
            "[LightGBM] [Info] Number of data points in the train set: 934, number of used features: 76\n",
            "[LightGBM] [Info] Start training from score 180918.995717\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000542 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3756\n",
            "[LightGBM] [Info] Number of data points in the train set: 934, number of used features: 75\n",
            "[LightGBM] [Info] Start training from score 179400.160600\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000528 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3776\n",
            "[LightGBM] [Info] Number of data points in the train set: 934, number of used features: 76\n",
            "[LightGBM] [Info] Start training from score 181057.426124\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000506 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3746\n",
            "[LightGBM] [Info] Number of data points in the train set: 935, number of used features: 75\n",
            "[LightGBM] [Info] Start training from score 177617.698396\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001066 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3732\n",
            "[LightGBM] [Info] Number of data points in the train set: 935, number of used features: 75\n",
            "[LightGBM] [Info] Start training from score 179264.774332\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000615 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3982\n",
            "[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 78\n",
            "[LightGBM] [Info] Start training from score 179651.292808\n",
            "Training base model 3/3\n",
            "Training base model 1/3\n",
            "Training base model 2/3\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000637 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3758\n",
            "[LightGBM] [Info] Number of data points in the train set: 934, number of used features: 74\n",
            "[LightGBM] [Info] Start training from score 182981.992505\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000468 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3759\n",
            "[LightGBM] [Info] Number of data points in the train set: 934, number of used features: 74\n",
            "[LightGBM] [Info] Start training from score 181315.827623\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000515 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3753\n",
            "[LightGBM] [Info] Number of data points in the train set: 934, number of used features: 74\n",
            "[LightGBM] [Info] Start training from score 178073.460385\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000546 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3746\n",
            "[LightGBM] [Info] Number of data points in the train set: 935, number of used features: 75\n",
            "[LightGBM] [Info] Start training from score 180976.968984\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000669 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3741\n",
            "[LightGBM] [Info] Number of data points in the train set: 935, number of used features: 74\n",
            "[LightGBM] [Info] Start training from score 182172.062032\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000999 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3968\n",
            "[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 76\n",
            "[LightGBM] [Info] Start training from score 181104.263699\n",
            "Training base model 3/3\n",
            "Training base model 1/3\n",
            "Training base model 2/3\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000512 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3761\n",
            "[LightGBM] [Info] Number of data points in the train set: 934, number of used features: 75\n",
            "[LightGBM] [Info] Start training from score 178116.791221\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000498 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3759\n",
            "[LightGBM] [Info] Number of data points in the train set: 934, number of used features: 74\n",
            "[LightGBM] [Info] Start training from score 182302.331906\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000504 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3755\n",
            "[LightGBM] [Info] Number of data points in the train set: 934, number of used features: 76\n",
            "[LightGBM] [Info] Start training from score 180338.490364\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000519 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3733\n",
            "[LightGBM] [Info] Number of data points in the train set: 935, number of used features: 74\n",
            "[LightGBM] [Info] Start training from score 182625.094118\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000462 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3751\n",
            "[LightGBM] [Info] Number of data points in the train set: 935, number of used features: 75\n",
            "[LightGBM] [Info] Start training from score 183248.866310\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000684 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3983\n",
            "[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 78\n",
            "[LightGBM] [Info] Start training from score 181327.004281\n",
            "Training base model 3/3\n",
            "Training base model 1/3\n",
            "Training base model 2/3\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000580 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3762\n",
            "[LightGBM] [Info] Number of data points in the train set: 934, number of used features: 75\n",
            "[LightGBM] [Info] Start training from score 180497.810493\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000572 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3733\n",
            "[LightGBM] [Info] Number of data points in the train set: 934, number of used features: 75\n",
            "[LightGBM] [Info] Start training from score 179816.622056\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000531 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3751\n",
            "[LightGBM] [Info] Number of data points in the train set: 934, number of used features: 75\n",
            "[LightGBM] [Info] Start training from score 182246.054604\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000498 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3776\n",
            "[LightGBM] [Info] Number of data points in the train set: 935, number of used features: 75\n",
            "[LightGBM] [Info] Start training from score 182753.671658\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000532 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3761\n",
            "[LightGBM] [Info] Number of data points in the train set: 935, number of used features: 74\n",
            "[LightGBM] [Info] Start training from score 180094.491979\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000564 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3984\n",
            "[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 77\n",
            "[LightGBM] [Info] Start training from score 181081.876712\n",
            "Training base model 3/3\n",
            "\n",
            "Model Scores:\n",
            "CatBoost:  0.120249\n",
            "XGBoost:   0.126613\n",
            "LightGBM:  0.132586\n",
            "Stacking:  0.123707\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#setting up our stacking model\n",
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "#base models list\n",
        "base_models = [\n",
        "    xgb_model,\n",
        "    lgb_model,\n",
        "    cat_model\n",
        "]\n",
        "\n",
        "#instantiating meta model: lasso this time.\n",
        "meta_model = Lasso(alpha=0.0005)\n",
        "\n",
        "#instantiate stacking ensemble\n",
        "stacker = StackingRegressor(base_models, meta_model)\n",
        "\n",
        "#evaluate it with cross validation\n",
        "def rmsle_cv(model, X, y, n_folds=5):\n",
        "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "    rmse = np.zeros(n_folds)\n",
        "\n",
        "    for i, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
        "        model.fit(X.iloc[train_idx], y.iloc[train_idx])\n",
        "        pred = model.predict(X.iloc[val_idx])\n",
        "        rmse[i] = np.sqrt(mean_squared_log_error(y.iloc[val_idx], pred))\n",
        "\n",
        "    return rmse.mean()\n",
        "stack_score = rmsle_cv(stacker, train_data, target)\n",
        "print(\"\\nModel Scores:\")\n",
        "#original scores\n",
        "print(f\"CatBoost:  {0.12024884970422434:.6f}\")\n",
        "print(f\"XGBoost:   {0.12661306520175142:.6f}\")\n",
        "print(f\"LightGBM:  {0.1325861852404577:.6f}\")\n",
        "#stacked model score\n",
        "print(f\"Stacking:  {stack_score:.6f}\")"
      ],
      "metadata": {
        "id": "ko2gaR8akjJN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}